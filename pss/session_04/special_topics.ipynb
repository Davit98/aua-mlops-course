{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6b3d7d1e-305d-47ed-81e2-2381e835c1b3",
   "metadata": {},
   "source": [
    "# AUA, DS 229 – MLOps\n",
    "### Week 5 – Special topics in data processing and model development\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef78bef2-6439-494d-93a9-436208993da3",
   "metadata": {},
   "source": [
    "Agenda:\n",
    "- Processing categorical and numerical variables\n",
    "- Handling missing data\n",
    "- Hyperparameter optimization with Optuna\n",
    "- Quantization of DNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f34ce47-8526-4bb4-baee-7d9ed3aea1c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install category-encoders\n",
    "# !pip install optuna\n",
    "# !pip install torchvision\n",
    "# !pip install xgboost\n",
    "# !pip install plotly==5.13.0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63113b7f-3f44-4cad-9220-12c82eeb4f25",
   "metadata": {},
   "source": [
    "### **Categorical variable encoding**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cf1b2f9-354c-48e1-884f-17b9c7dbd557",
   "metadata": {},
   "source": [
    "In data science, before starting the modeling process, data preparation is a necessary step. There are several tasks that need to be performed during this stage, one of which is encoding categorical data. This is considered critical because in real life, most of the data comes in the form of categorical string values, while most machine learning models only work with integer or other numerical values that can be understood by the model. The reason for this is that mathematical operations, which are at the heart of all models, can only be performed using numerical data, not strings or any other data types. The numbers used can be either floating-point or integer values."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2e1e9d0-1605-441a-802f-aea0d57a4454",
   "metadata": {},
   "source": [
    "Encoding categorical data is a process of converting categorical data into integer format so that the data with converted categorical values can be provided to the models. In this section, we will discuss categorical data encoding algorithms and will try to understand why we need the process of categorical data encoding. \n",
    "\n",
    "Categorical variables can be divided into two categories: **Nominal (no particular order)** and **Ordinal (some ordered)**.\n",
    "<center><img src=\"images/categorical_data.png\" width=\"900\" height=\"900\"/></center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52524592-1464-4914-8376-9572ae896d87",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "original_data = pd.DataFrame({\n",
    "    \"color\": [\"red\", \"red\", \"blue\", \"orange\", \"red\", \"blue\", \"blue\", \"red\", \"orange\", \"blue\"],\n",
    "    \"condition\": [\"Very good\", \"Good\", \"Medium\", \"Very good\", \"Bad\", \"Medium\", \"Bad\", \"Bad\", \"Medium\", \"Good\"],\n",
    "    \"mileage\": [120000, 90000, 150000, 200000, 130000, 145000, 95000, 70000, 105000, 65000],\n",
    "    \"price\": [10.1, 8.5, 6.3, 9.0, 12.2, 14.5, 10.8, 11.8, 12.4, 7.5]\n",
    "})\n",
    "\n",
    "original_data.head(n=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61e7a4cf-a82d-440e-8fec-6bb2d3bf58b4",
   "metadata": {},
   "source": [
    "#### 1) Label Encoding\n",
    "\n",
    "Under this encoding scheme, every category is given a numerical value ranging from 1 to N (where N is the total number of categories for that feature). However, a key drawback of this method is that there is no inherent relationship or order between the categories (assigned labels), though a ML algorithm may mistakenly interpret them as having some form of order or connection. For example, in the case of attribute **color** we may have `\"red\" < \"blue\" < \"orange\"` with the corresponding assignment of labels 1, 2 and 3. \n",
    "\n",
    "For the implementation either use `factorize` function from **pandas** or `LabelEncoder` from **sklearn**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1c66010-2bd4-48d7-94ea-f1644740df34",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = original_data.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83a03479-d0eb-4ead-8bb0-4e08d20a07b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.loc[:, \"color_labels\"] = pd.factorize(data[\"color\"])[0].reshape(-1, 1)\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6d402de-7c5c-4cd5-a517-169358f2b3ae",
   "metadata": {},
   "source": [
    "#### 2) One Hot Encoding\n",
    "\n",
    "This approach involves assigning a vector consisting of 1s and 0s to each category, representing the presence or absence of that feature. The number of vectors required corresponds to the number of categories for the features. However, if there are a large number of categories for a particular feature, this can result in a significant slowdown in the learning process due to the large number of resulting columns.\n",
    "\n",
    "The desired outcome can be achieved using either `get_dummies` function from **pandas** or `OneHotEncoder` from **sklearn**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d253504-e5d9-4d9e-a01a-4d9ee1a29232",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = original_data.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4688f1d-d928-42a3-8822-b2f1e997bb58",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.get_dummies(data, prefix=[\"color\"], columns=[\"color\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a264c9c-8166-40d9-bcf1-42ace38a8193",
   "metadata": {},
   "source": [
    "#### 3) Ordinal Encoding\n",
    "\n",
    "Ordinal encoding is used to maintain the ordinal nature of variables in their encoding process, but it should only be applied to variables that are inherently ordinal. This encoding method is similar to label encoding, but it differs in that it takes into account the ordinal nature of the variable, whereas label encoding does not make this distinction and simply assigns a sequence of integers.\n",
    "\n",
    "For the implementation, one needs to define the mapping between categories and labels manually. Labels can be assigned using `pandas.Series.map` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c40f048-1801-441c-98de-9e9b7aa3a7f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = original_data.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7eb23e7-7adb-4cac-8542-0cd31ac76155",
   "metadata": {},
   "outputs": [],
   "source": [
    "temperature_mapping = {\n",
    "    \"Bad\": 1, \n",
    "    \"Medium\": 2, \n",
    "    \"Good\": 3, \n",
    "    \"Very good\": 4\n",
    "}\n",
    "data[\"condition_labels\"] = data[\"condition\"].map(temperature_mapping)\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8cbc4c0-f870-4c8b-91a9-32813f7f0230",
   "metadata": {},
   "source": [
    "#### 4) Binary Encoding\n",
    "\n",
    "Binary encoding transforms a category into a series of binary digits, with each digit creating a separate feature column. The number of resulting features generated by binary encoding is equal to $$⌈\\log_2 n⌉,$$ where $n$ represents the number of unique categories. In the given example, there are four features, which would result in three binary-encoded features. In contrast to One Hot Encoding, binary encoding requires fewer feature columns. For instance, if there were 100 categories, One Hot Encoding would require 100 features, while binary encoding would only require seven features. There are 3 steps to apply the encoding:\n",
    "1) The categories are first converted to numeric order starting from 1 (order is created as categories appear in the data and **do not mean any ordinal nature**).\n",
    "2) The assigned integers are converted into binary code, so for example, 3 becomes 11.\n",
    "3) Then the digits of the binary number form separate columns.\n",
    "\n",
    "For the implementation we will use [category_encoders](https://github.com/scikit-learn-contrib/category_encoders) package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e615e5a-f923-464d-bdcc-b3627fd7a50b",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = original_data.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5c5d3b4-eb54-4581-8844-349d0af80ce5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from category_encoders import BinaryEncoder\n",
    "\n",
    "\n",
    "encoder = BinaryEncoder(cols=[\"color\"])\n",
    "color_bin = encoder.fit_transform(data[\"color\"])\n",
    "data = pd.concat([data, color_bin], axis=1)\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccfb0146-03bc-4a73-9d6b-67a2ed368cc1",
   "metadata": {},
   "source": [
    "#### 5) Mean Encoding\n",
    "\n",
    "Mean encoding shares similarities with label encoding, but instead of assigning labels arbitrarily, they are directly linked to the target variable. In this encoding method, the label assigned to each category is determined by the mean value of the target variable in the training data for that category. Mean encoding helps to reveal relationships between similar categories, but these relationships are limited to the categories and the target variable. The benefits of mean encoding include its ability to expedite the learning process and its minimal impact on data volume. However, mean encoding is often associated with overfitting and requires regularization techniques such as cross-validation. To apply a mean encoding to a categorcial variable first of all one needs to group by that categorical variable and pick the mean of 'target' fo each category."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d42c775-ce35-4ac6-9223-9665434bbad5",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = original_data.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "209f30e5-1b7f-43ac-9fcc-721d655a6ee5",
   "metadata": {},
   "outputs": [],
   "source": [
    "target = \"price\"  # Consider the target to be 'price'.\n",
    "\n",
    "mean_enc_map = data.groupby(\"color\")[target].mean()\n",
    "data.loc[:, \"color_enc\"] = data[\"color\"].map(mean_enc_map)\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfe184c5-d4ab-43f6-bb1e-14a2a234bf72",
   "metadata": {},
   "source": [
    "There are other encodings, such as **Probability Ratio Encoding**, **James-Stein Encoding**, **M-estimator Encoding**, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9185148d-e1bf-4c3f-b64e-fcf706f03f71",
   "metadata": {},
   "source": [
    "### **Numerical variable encoding**\n",
    "\n",
    "When you're processing data, it's crucial to ensure that your features are within comparable ranges, which is called feature scaling. This is a straightforward task that can significantly enhance your model's performance, and failing to do so can lead to nonsensical predictions, particularly when using traditional algorithms such as gradient-boosted trees and logistic regression."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d990f5c-e6f8-408f-aca5-ddfe5307c7d4",
   "metadata": {},
   "source": [
    "#### 1) Min-max scaling\n",
    "\n",
    "A simple way to scale your features is to get each feature to be in the range $[0, 1]$. Given a variable $x$, its values can be rescaled to be in this range using the following formula:\n",
    "$$\n",
    "x' = \\frac{x-\\min(x)}{\\max(x)-\\min(x)}\n",
    "$$\n",
    "The scaling can be implemented with either pure **pandas** or with `MinMaxScaler` from **sklearn**.  \n",
    "\n",
    "P.S.  \n",
    "If you want your feature to be in the range `[a, b]` use the following modification:\n",
    "$$x' = a + \\frac{(x - \\min(x))(b - a)}{\\max(x) - \\min(x)}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00048d53-3aa6-4e16-9b67-85c6d3e50242",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = original_data.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1b970e9-7253-48e4-9302-e3fc0de63820",
   "metadata": {},
   "outputs": [],
   "source": [
    "for feature in (\"price\", \"mileage\"):\n",
    "    min_value, max_value = data[feature].min(), data[feature].max()\n",
    "    data.loc[:, f\"{feature}_scaled\"] = (data[feature] - min_value) / (max_value - min_value)\n",
    "    \n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dad399fa-ae49-4b37-84a2-d38ae94aea16",
   "metadata": {},
   "outputs": [],
   "source": [
    "data[\"price_scaled\"].min(), data[\"price_scaled\"].max()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97e3d39f-a0af-4823-98c5-b31052d7c55b",
   "metadata": {},
   "source": [
    "#### 2) Standard scaling (Normalization)\n",
    "\n",
    "If you think that your variables might follow a normal distribution, it might be helpful to normalize them so that they have zero-mean and unit variance. This process is called standardization.\n",
    "$$\n",
    "x' = \\frac{x - \\bar{x}}{\\sigma}\n",
    "$$\n",
    "\n",
    "The scaling can be implemented with either pure **pandas** or with `StandardScaler` from **sklearn**.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5baf391c-86b9-429e-ac86-3183269b023d",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = original_data.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b90bb6ea-c971-40c4-93e9-ff5f1c7365ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "for feature in (\"price\", \"mileage\"):\n",
    "    mean_value, std_value = data[feature].mean(), data[feature].std()\n",
    "    data.loc[:, f\"{feature}_scaled\"] = (data[feature] - mean_value) / std_value\n",
    "    \n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "968e3a33-76f1-4a0c-9e73-1d487c682409",
   "metadata": {},
   "outputs": [],
   "source": [
    "data[\"price_scaled\"].mean(), data[\"price_scaled\"].std()  # 0 mean and unit variance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1afcf8a-283d-4cd7-aecc-8790c56d6209",
   "metadata": {},
   "source": [
    "### Handling missing values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80d16c53-f5ba-4e72-bf1c-ae2c04236ad4",
   "metadata": {},
   "source": [
    "One of the first things you might notice when dealing with data in production is that some values are missing. A crucial fact is that not all types of missing values are equal. Let's elaborate on this point. Consider this toy dataset.\n",
    "\n",
    "| ID | Age | Gender | Annual income | Marital status | Number of children | Job | Buy? |\n",
    "| --- | --- | --- | --- | --- | --- | --- | --- |\n",
    "| 1 | ? | A | 150000 | ? | 1 | Engineer | No |\n",
    "| 2 | 27  | B | 50000 | ? | ? | Teacher | No |\n",
    "| 3 | ? | A | 100000 | Married | 2 | ? | Yes |\n",
    "| 4 | 40 | B | ? | ? | 2 | Engineer | Yes |\n",
    "| 5 | 35  | B | ? | Single | 0 | Doctor | Yes |\n",
    "| 6 | ? | A | 50000 | ? | 0 | Teacher | No |\n",
    "| 7 | 33 | B | 60000 | Single | ? | Teacher | No |\n",
    "| 8 | 20 | B | 10000 | ? | ? | Student | No |\n",
    "\n",
    "\n",
    "There are 3 types of missing values:\n",
    "1) **Missing not at random**: when the reason a value is missing is because of the value itself. In this example, we might notice that respondents of gender “B” with higher income tend not to disclose their income (the have reported only low income values). The income values are missing for reasons related to the values themselves.\n",
    "2) **Missing at random**: when the reason a value is missing is not due to the value itself, but due to another observed variable. In this example, we might notice that age values are often missing for respondents of the gender “A”, which might be because the people of gender A in this survey don’t like disclosing their age.\n",
    "3) **Missing completely at random**: when there’s no pattern in when the value is missing. In this example, we might think that the missing values for the column “Job” might be completely random, not because of the job itself and not because of another variable. People just forget to fill in that value sometimes for no particular reason. However, this type of missing is very rare. There are usually reasons why certain values are missing, and you should investigate.\n",
    "\n",
    "When encountering missing values, you can either fill in the missing values with certain values (**imputation**), or remove the missing values (**deletion**)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aad69482-1e90-4c5a-b88a-6c2c758b07e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {\n",
    "    \"ID\": [1, 2, 3, 4, 5, 6, 7, 8],\n",
    "    \"Age\": [\"?\", 27, \"?\", 40, 35, \"?\", 33, 20], \n",
    "    \"Gender\": [\"A\", \"B\", \"A\", \"B\", \"B\", \"A\", \"B\", \"B\"],\n",
    "    \"Annual income\": [150000, 50000, 100000, \"?\", \"?\", 50000, 60000, 10000],\n",
    "    \"Marital status\": [\"?\", \"?\", \"Married\", \"?\", \"Single\", \"?\", \"Single\", \"?\"],\n",
    "    \"Number of children\": [1, \"?\", 2, 2, 0, 0, \"?\", \"?\"],\n",
    "    \"Job\": [\"Engineer\", \"Teacher\", \"?\", \"Engineer\", \"Doctor\", \"Teacher\", \"Teacher\", \"Student\"],\n",
    "    \"Buy?\": [\"No\", \"No\", \"Yes\", \"Yes\", \"Yes\", \"No\", \"No\", \"No\"]\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f14cda35-3bbf-4c7e-803d-b6b5966a89f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.replace(\"?\", np.NaN, inplace=True)  # In order for pandas to recognize missing values.\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "322633b4-57ae-4528-a4a3-12b6163a9da7",
   "metadata": {},
   "source": [
    "#### 1) Deletion\n",
    "\n",
    "The simplest way to handle missing values is to delete them. There are two types of deleteion: row-wise and column-wise."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7c44883-75f7-4cf8-81b7-520c667150d4",
   "metadata": {},
   "source": [
    "**Column-wise deletion**:\n",
    "If a variable has too many missing values, just remove that variable. For example, in the example above, over 50% of the values for the variable “Marital status” are missing, so you might be tempted to remove this variable from your model. The drawback of this approach is that you might remove important information and reduce the accuracy of your model. Marital status might be highly correlated to buying houses, as married couples are much more likely to be homeowners than single people."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f185737e-ce10-4e3f-b99a-792b2eab5cd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"Marital status\"].isna().mean() * 100  # The percentage of missing values in 'Marital status'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a00826ff-a27d-4249-bfb0-e508992da61e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(columns=[\"Marital status\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6b69d70-6f7f-4473-9865-95044c638aaf",
   "metadata": {},
   "source": [
    "**Row-wise deletion**:\n",
    "If an example has missing value(s), just remove that example from the data. **This method can work when the missing values are completely at random and the number of examples with missing values is small**, such as less than 0.1%. You don’t want to do row deletion if that means 10% of your data examples are removed.  \n",
    "However, removing rows of data can also remove important information that your model needs to make predictions, especially if the missing values are not at random. For example,  you don’t want to remove examples of gender B respondents with missing income because whether income is missing is information itself (missing income might mean higher income, and thus, more correlated to buying a house) and can be used to make predictions.  \n",
    "On top of that, removing rows of data can create biases in your model, especially if the missing values are at random. For example, if you remove all examples missing age values in the data, you will remove all respondents with gender A from your data, and your model won’t be able to make predictions for respondents with gender A."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96c54773-bc33-40f5-95d6-8ca0d92a318b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dropna(axis=0, subset=[\"Job\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6abdaceb-f78e-4e9d-beff-df93469f6ece",
   "metadata": {},
   "source": [
    "#### 2) Imputation\n",
    "\n",
    "While it may be tempting to simply delete data that is missing, doing so can result in the loss of crucial information or introduce bias into your model. If you prefer not to delete missing values, you'll need to fill them in through a process known as imputation. However, the difficult part is determining which specific values to use for the imputation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81aac999-bd1d-467e-ba55-e023a343acea",
   "metadata": {},
   "source": [
    "One common practice is to fill in missing values with their defaults. For example, if the job is missing, you might fill it with an empty string `“”`. Another common practice is to fill in missing values with the mean, median, or mode (the most common value). For example, if the temperature value is missing for a data example whose month value is July, it’s not a bad idea to fill it with the median temperature of July."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f034e364-20e6-4fb0-ac7b-a4540fcf1f4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_value_map = {\n",
    "    \"Job\": \"\"\n",
    "}\n",
    "df.fillna(missing_value_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45f81790-e08b-4ed4-a97f-b58f75b2922d",
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_value_map = {\n",
    "    \"Annual income\": df[\"Annual income\"].median()\n",
    "}\n",
    "df.fillna(missing_value_map)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68ba68d1-1171-4578-bd73-14db6b520c44",
   "metadata": {},
   "source": [
    "#### 3) Bonus 😀 \n",
    "\n",
    "Impute missing values with one of the imputation methods (e.g. mean imputation) and create a binary column where 1s will repsent missing values. This provides a model an opportunity to decide on handing missing values on its own."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "096f74c3-3bfe-42e0-84d4-213490633a96",
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_value_map = {\n",
    "    \"Annual income\": df[\"Annual income\"].mean()\n",
    "}\n",
    "df[\"Missing Annual income\"] = df[\"Annual income\"].isna().astype(int)\n",
    "df.fillna(missing_value_map)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55573b11-9552-478b-97e1-359ee07d0fac",
   "metadata": {},
   "source": [
    "**IMPORTANT NOTE**  \n",
    "Some algorithms, however, support handling missing values inherently. Examples are **XGBoost**, **LightGBM**, **CatBoost**. Moreover, **Catboost** and **LightGBM** can also handle categorical variables (you need to specify categorical variables in the model initiaization step) while **XGBoost** cannot as it expcts only numerical values. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf0d954b-64e0-40f2-a294-a3de506eb2a6",
   "metadata": {},
   "source": [
    "### **Hyperparameter optimization with** [**Optuna**](https://optuna.org/)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "623e2880-ee27-4e14-ada0-aa8e5b61e5df",
   "metadata": {},
   "source": [
    "Optuna is a hyperparameter optimization software framework, particularly designed for machine learning pipelines. Optuna enables users to adopt state-of-the-art algorithms for sampling hyperparameters and pruning unpromising trials. This helps to speed up optimization time and performance greatly compared to traditional methods such as GridSearch.\n",
    "\n",
    "A hyperparameter is a parameter whose value is used to control the learning process (but is not learnt/adjusted during the learning process).\n",
    "\n",
    "The problem with (hyperparamter) optimization is that, often the search space can be indefinite and the budget to perform this search is constrained. Therefore, to search for the global minimum efficiently, an algorithm has to implement ways to utilize the budget efficiently. The two most commonly used algorithms are the following:  \n",
    "- **Grid search** - exhaustively searches through a predefined subset.\n",
    "- **Random search** - selects randomly from a predefined subset.\n",
    "\n",
    "More advanced family of methods is Bayesian optimization.\n",
    "\n",
    "**Bayesian optimization**  \n",
    "Bayesian optimization methods search for global optimization by iteratively building a probabilistic model of the function mapping from hyperparameter values to the objective function. The probabilistic model captures beliefs about the behavior of the function to form a posterior distribution over the objective function. After that, the posterior distribution is further used to form an acquisition function that determines the next point with the best improvement probability.  \n",
    "Multiple algorithms are then proposed to optimally balance the exploration-exploitation dilemma. Some examples are: *Tree-structured Parzen Estimator (TPE)* and *Gaussian Process Regressor*."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14e0f7d5-7cb2-44bc-9d85-3caa0ca5ee21",
   "metadata": {},
   "source": [
    "**By default, Optuna implements a Bayesian optimization algorithm (TPE) but it can be easily switched to other supported algorithms in the package**.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3247ae83-156c-42a1-be36-5af12ae02107",
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import XGBClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.model_selection import cross_val_score\n",
    "import optuna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86fcc7d9-731b-4d2f-991d-1bbdeee00fb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = load_breast_cancer(return_X_y=True, as_frame=True)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "008bee8f-d0ca-479f-8fbd-be9da469a5dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = XGBClassifier()\n",
    "model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07783bd0-e79c-4025-81f4-5efaf7c3e1cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Accuracy: {round(accuracy * 100.0, 2)}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "391985d4-8dca-45a8-b2c3-b2d65423bce6",
   "metadata": {},
   "source": [
    "First of all, Optuna requires us to define an **objective function**.\n",
    "The objective function will contain the entire logic of a regular model definition, training and testing process. After the model evaluation, it should return the evaluation metric which is also picked by the user."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54521914-12f6-4d8c-a23e-b3fb37e413f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.4, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe01880e-668f-450e-9d00-f3cd0637bcee",
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(trial):\n",
    "    \n",
    "    # 1) Define the hyperparameter set and the corresponding search regions.\n",
    "    # You can also specify parameters than should be fixed.\n",
    "    params = {\n",
    "        \"max_depth\": trial.suggest_int(\"max_depth\", 1, 9),\n",
    "        \"learning_rate\": trial.suggest_float(\"learning_rate\", 0.01, 1.0),\n",
    "        \"n_estimators\": trial.suggest_int(\"n_estimators\", 50, 500)\n",
    "    }\n",
    "    \n",
    "    # 2) Fit the model.\n",
    "    optuna_model = XGBClassifier(**params)  # XGBClassifier(max_depth=`suggested value`, ...)\n",
    "    optuna_model.fit(X_train, y_train)\n",
    "    \n",
    "    # 3) Evaluate the model on validation set.\n",
    "    # In case you data is big and validation set is representative, use it \n",
    "    # without cross validation.\n",
    "    score = cross_val_score(optuna_model, X_val, y_val, cv=3).mean()  # Returns mean accuracy score over folds.\n",
    "    \n",
    "    # 4) Return a score summarizing the trial.\n",
    "    return score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "389d750c-a7a3-42f8-ba9b-8b546c6ba612",
   "metadata": {},
   "source": [
    "The **Trial** class will be used to store information of one specific combination of hyperparameters later used by the machine learning model.\n",
    "\n",
    "A **Study** object can then be called to optimize the objective function to find the best hyperparameters combination. It will then run trials iteratively until a user-defined maximum trial or time. The trial with the best hyperparameters will be stored in `study.best_trial`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d85dbf0-0fe7-4604-9c4a-084bcb79c509",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a study.\n",
    "study = optuna.create_study(direction=\"maximize\")  # We aim to maximize the accuracy (score) returned by objective function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75cfe069-72e0-4d62-b0ec-74cbeed64077",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start optimization.\n",
    "study.optimize(objective, n_trials=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cba866a6-c4d7-45ef-a520-04b41434a048",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Number of finished trials: {len(study.trials)}\", \"Best trial:\", sep=\"\\n\")\n",
    "best_trial = study.best_trial  # Selecting the trial with the highest score.\n",
    "print(f\"  Value: {best_trial.value}\", \"  Params: \", sep=\"\\n\")\n",
    "\n",
    "for key, value in best_trial.params.items():\n",
    "    print(f\"    {key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "262e6b72-473a-44c0-8b57-5402120e8a5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "optuna.visualization.plot_contour(study, params=[\"learning_rate\", \"n_estimators\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c91f91e-1152-4c6e-bafb-d7a4e68b56ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "optuna.visualization.plot_param_importances(study)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "559f072e-6197-4723-8047-f0a00eba070e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrain the model with best hyperparamters.\n",
    "params = best_trial.params\n",
    "model = XGBClassifier(**params)\n",
    "model.fit(pd.concat([X_train, X_val]), pd.concat([y_train, y_val]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aab82478-eb99-4da3-ab31-8b1a6cf7b657",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Accuracy before: {round(accuracy * 100.0, 2)}%\")\n",
    "\n",
    "y_pred = model.predict(X_test)\n",
    "accuracy_tuned = accuracy_score(y_test, y_pred)\n",
    "print(f\"Accuracy after: {round(accuracy_tuned * 100.0, 2)}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21761437-b555-41af-b785-da0452a95a8a",
   "metadata": {},
   "source": [
    "Optuna employs sophisticated hyperparameter optimization algorithms, which are designed to quickly find the optimal objective when it's too expensive to repeatedly train the model. However, it's important to note that when using a less complex model with a small amount of data, GridSearch and RandomSearch may actually outperform these advanced algorithms in terms of both speed and performance. Nevertheless, as the amount of data grows and the models become more intricate, the cost of random hyperparameter tuning increases significantly, making these advanced algorithms highly effective and efficient."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "249058f9-e2e6-4c9d-8390-fd18574c5659",
   "metadata": {
    "tags": []
   },
   "source": [
    "### **Quantization of neural networks**\n",
    "\n",
    "<center><img src=\"images/quantization.png\" width=\"500\" height=\"500\"/></center>  \n",
    "\n",
    "[[Image source]](https://developer.nvidia.com/blog/achieving-fp32-accuracy-for-int8-inference-using-quantization-aware-training-with-tensorrt/)\n",
    "\n",
    "Quantization refers to techniques for doing both computations and memory accesses with lower precision data, usually `int8` compared to floating point implementations. Quantization does not however come without additional cost. Fundamentally quantization means introducing approximations and the resulting networks have slightly less accuracy. These techniques attempt to minimize the gap between the full floating point accuracy and the quantized accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fddbd7d1-0143-4774-99c0-c3da6c782944",
   "metadata": {},
   "source": [
    "**The three modes of Quantiation supported in PyTorch starting version 1.3**\n",
    "\n",
    "1) **Dynamic Quantization**  \n",
    "This involves not just converting the weights to int8 - as happens in all quantization variants - but also converting the activations to int8 on the fly, just before doing the computation (hence “dynamic”). The computations will thus be performed using efficient int8 matrix multiplication and convolution implementations, resulting in faster compute. However, the activations are read and written to memory in floating point format.\n",
    "\n",
    "2) **Post-Training Static Quantization**  \n",
    "One can further improve the performance (latency) by converting networks to use both integer arithmetic and int8 memory accesses. Static quantization performs the additional step of first feeding batches of data through the network and computing the resulting distributions of the different activations (specifically, this is done by inserting “observer” modules at different points that record these distributions). This information is used to determine how specifically the different activations should be quantized at inference time. Importantly, this additional step allows us to pass quantized values between operations instead of converting these values to floats - and then back to ints - between every operation, resulting in a significant speed-up.\n",
    "\n",
    "3) **Quantization Aware Training (QAT)**  \n",
    "This method typically results in highest accuracy of these three. With QAT, all weights and activations are “fake quantized” during both the forward and backward passes of training: that is, float values are rounded to mimic int8 values, but all computations are still done with floating point numbers. Thus, all the weight adjustments during training are made while “aware” of the fact that the model will ultimately be quantized; after quantizing, therefore, this method usually yields higher accuracy than the other two methods.\n",
    "\n",
    "<center><img src=\"images/quantization_types.png\" width=\"900\" height=\"900\"/></center>\n",
    "\n",
    "[[Source]](https://pytorch.org/blog/introduction-to-quantization-on-pytorch/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fa09bff-fd41-4f68-a2cf-5051059cc942",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebbdacc2-22fb-413e-a89a-9c8ae6e363dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Source: https://github.com/pytorch/examples/blob/main/mnist/main.py\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 32, 3, 1)\n",
    "        self.relu1 = torch.nn.ReLU()\n",
    "        self.conv2 = nn.Conv2d(32, 64, 3, 1)\n",
    "        self.relu2 = nn.ReLU()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.max_pool2d = nn.MaxPool2d(2)\n",
    "        self.dropout1 = nn.Dropout(0.25)\n",
    "        self.dropout2 = nn.Dropout(0.5)\n",
    "        self.fc1 = nn.Linear(9216, 128)\n",
    "        self.relu3 = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(128, 10)\n",
    "\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.relu1(x)\n",
    "        \n",
    "        x = self.conv2(x)\n",
    "        x = self.relu2(x)\n",
    "        \n",
    "        x = self.max_pool2d(x)\n",
    "        x = self.dropout1(x)\n",
    "        x = self.flatten(x)\n",
    "        \n",
    "        x = self.fc1(x)\n",
    "        x = self.relu3(x)\n",
    "        \n",
    "        x = self.dropout2(x)\n",
    "        x = self.fc2(x)\n",
    "        \n",
    "        output = F.log_softmax(x, dim=1)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0e36f2e-d6f2-48a1-95c9-58495429162d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Source: https://github.com/pytorch/examples/blob/main/mnist/main.py\n",
    "\n",
    "def train(model, device, train_loader, optimizer, epoch, log_interval=50):\n",
    "    model.train()\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = F.nll_loss(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if batch_idx % log_interval == 0:\n",
    "            print(\"Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}\".format(\n",
    "                epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "                100. * batch_idx / len(train_loader), loss.item()))\n",
    "\n",
    "\n",
    "def test(model, device, test_loader):\n",
    "    model.eval()\n",
    "    test_loss, correct = 0, 0\n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output = model(data)\n",
    "            test_loss += F.nll_loss(output, target, reduction='sum').item()\n",
    "            pred = output.argmax(dim=1, keepdim=True)\n",
    "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "\n",
    "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "        test_loss, correct, len(test_loader.dataset),\n",
    "        100. * correct / len(test_loader.dataset)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f50931f-7996-4562-9612-92822409e107",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Source: https://github.com/pytorch/examples/blob/main/mnist/main.py\n",
    "\n",
    "torch.manual_seed(0)\n",
    "device = torch.device(\"cpu\")\n",
    "\n",
    "epochs = 1\n",
    "transform=transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.1307,), (0.3081,))])\n",
    "\n",
    "train_data = datasets.MNIST('../data', train=True, download=True, transform=transform)\n",
    "test_data = datasets.MNIST('../data', train=False, transform=transform)\n",
    "train_loader = torch.utils.data.DataLoader(train_data, batch_size=64)\n",
    "test_loader = torch.utils.data.DataLoader(test_data, batch_size=64)\n",
    "\n",
    "model = Net().to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-5)\n",
    "\n",
    "for epoch in range(1, epochs + 1):\n",
    "    train(model, device, train_loader, optimizer, epoch)\n",
    "    optimizer.step()\n",
    "\n",
    "torch.save(model.state_dict(), \"mnist_cnn.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d80b1429-68a3-4d8d-8928-0bdb2ca57fda",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Net().to(device)\n",
    "model.load_state_dict(torch.load(\"mnist_cnn.pt\"))\n",
    "model.eval()\n",
    "\n",
    "test(model, device, test_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a268b4fe-4fb4-47dc-b61f-2e54cf9ed274",
   "metadata": {},
   "source": [
    "Now let's measure the total inference time of our model over the test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f0aaf80-df0d-447c-b4c5-7c0702596e79",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.profiler import profile, record_function, ProfilerActivity\n",
    "\n",
    "\n",
    "with profile(activities=[ProfilerActivity.CPU], record_shapes=True) as prof:\n",
    "    with record_function(\"model_inference\"):\n",
    "        test(model, device, test_loader)\n",
    "        \n",
    "print(prof.key_averages().table(sort_by=\"cpu_time_total\", row_limit=5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e50db6a-6b7e-492f-83cd-55b9adfb406c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print parameters of our network.\n",
    "for param_name, param in model.named_parameters():\n",
    "    print(f\"{param_name}: {param.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e371e003-68b4-496d-9c47-a51f3b3d3256",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose a weight matrix at random and observe that weight values are all floats.\n",
    "weight = dict(model.named_parameters())[\"conv1.weight\"]\n",
    "weight[0][0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74d28bde-3883-44df-87b2-2b985fa02a0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Moreover, they are stored in 32 bits.\n",
    "print(f\"Weights have a type of {weight.dtype}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8cbe4e3-70b2-4764-84b6-98c2d9a4e1b2",
   "metadata": {},
   "source": [
    "We will apply **Post Training Static Quantization** to our network.  \n",
    "Post Training Static Quantization (PTQ static) quantizes the weights and activations of the model. It fuses activations into preceding layers where possible. It requires calibration with a representative dataset to determine optimal quantization parameters for activations. Post Training Static Quantization is typically used when both memory bandwidth and compute savings are important with CNNs being a typical use case.\n",
    "\n",
    "[[Source]](https://pytorch.org/docs/stable/quantization.html#post-training-static-quantization)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b826b64a-fed1-47fa-a061-b2430a4f99b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        \n",
    "        self.quant = torch.quantization.QuantStub()  # QuantStub converts tensors from floating point to quantized.\n",
    "        \n",
    "        self.conv1 = nn.Conv2d(1, 32, 3, 1)\n",
    "        self.relu1 = torch.nn.ReLU()\n",
    "        self.conv2 = nn.Conv2d(32, 64, 3, 1)\n",
    "        self.relu2 = nn.ReLU()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.max_pool2d = nn.MaxPool2d(2)\n",
    "        self.dropout1 = nn.Dropout(0.25)\n",
    "        self.dropout2 = nn.Dropout(0.5)\n",
    "        self.fc1 = nn.Linear(9216, 128)\n",
    "        self.relu3 = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(128, 10)\n",
    "        \n",
    "        self.dequant = torch.quantization.DeQuantStub()  # DeQuantStub converts tensors from quantized to floating point\n",
    "\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Manually specify where tensors will be converted from floating point to quantized in the quantized model.\n",
    "        x = self.quant(x)\n",
    "        \n",
    "        x = self.conv1(x)\n",
    "        x = self.relu1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.relu2(x)\n",
    "        x = self.max_pool2d(x)\n",
    "        x = self.dropout1(x)\n",
    "        x = self.flatten(x)\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu3(x)\n",
    "        x = self.dropout2(x)\n",
    "        x = self.fc2(x)\n",
    "        \n",
    "        # Manually specify where tensors will be converted from quantized to floating point in the quantized model.\n",
    "        x = self.dequant(x)\n",
    "        \n",
    "        output = F.log_softmax(x, dim=1)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "707bea02-55b9-4b81-9cb1-3653b5d7de2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make sure the network's performane is the same.\n",
    "model = Net().to(device)\n",
    "model.load_state_dict(torch.load(\"mnist_cnn.pt\"))\n",
    "model.eval()\n",
    "test(model, device, test_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "949ec04d-0b98-4f99-a0bd-f7624c9dd880",
   "metadata": {},
   "source": [
    "Let's quantize our network!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b7529a1-c0ef-4538-a31c-930bec40610a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Net().to(device)\n",
    "model.load_state_dict(torch.load(\"mnist_cnn.pt\"))\n",
    "model.eval()  # Model must be set to eval mode for static quantization logic to work.\n",
    "\n",
    "# Attach a global qconfig, which contains information about what kind of observers to attach. \n",
    "# Use 'fbgemm' for server inference and 'qnnpack' for mobile inference.\n",
    "model.qconfig = torch.quantization.get_default_qconfig(\"fbgemm\")\n",
    "\n",
    "# Fuse the activations to preceding layers, where applicable. This needs to be done manually depending on the model architecture.\n",
    "# Common fusions include `conv + relu` and `conv + batchnorm + relu`.\n",
    "model_fused = torch.quantization.fuse_modules(model, [[\"conv1\", \"relu1\"], [\"conv2\", \"relu2\"], [\"fc1\", \"relu3\"]])\n",
    "\n",
    "# Prepare the model for static quantization. This inserts observers in the model that will \n",
    "# observe activation tensors during calibration.\n",
    "model_prepared = torch.quantization.prepare(model_fused)\n",
    "\n",
    "# Calibrate the prepared model to determine quantization parameters for activations in \n",
    "# a real world setting, the calibration would be done with a representative dataset.\n",
    "input_fp32, _ = next(iter(train_loader))  # Get the first batch of data.\n",
    "model_prepared(input_fp32)\n",
    "\n",
    "# Convert the observed model to a quantized model. This does several things:\n",
    "# 1) quantizes the weights, \n",
    "# 2) computes and stores the scale and bias value to be used with each activation tensor, \n",
    "# 3) replaces key operators with quantized implementations.\n",
    "model_int8 = torch.quantization.convert(model_prepared)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9986f01c-a92d-4be0-a34c-f634170eb82d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now let's measure the inference time performance of the quantized model.\n",
    "with profile(activities=[ProfilerActivity.CPU], record_shapes=True) as prof:\n",
    "    with record_function(\"model_inference\"):\n",
    "        test(model_int8, device, test_loader)\n",
    "        \n",
    "print(prof.key_averages().table(sort_by=\"cpu_time_total\", row_limit=5))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c61343bd-c3a4-4f13-92aa-93794300fc02",
   "metadata": {},
   "source": [
    "**We observe significant speed-up in inference time!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0e95710-6bb9-4a9b-aa0e-c55aab47fb9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_int8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26ba0410-3553-4670-b4ac-05e4ccfd2302",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check that weights are converted into 8-bit integers.\n",
    "print(f\"Before: {weight[0][0][0].data}\")\n",
    "print(f\"After: {model_int8.conv1.weight().int_repr()[0][0][0].data}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0aa312cc-432b-41fa-a5c3-bce2d9ce251f",
   "metadata": {},
   "source": [
    "Some performance results:\n",
    "<center><img src=\"images/quantization_results.png\" width=\"900\" height=\"900\"/></center>\n",
    "\n",
    "[[Source]](https://pytorch.org/blog/introduction-to-quantization-on-pytorch/)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb806679-f2e4-4606-8fbb-11f808ae2514",
   "metadata": {},
   "source": [
    "# References\n",
    "- [CS329S: Lecture 4. Feature Engineering](https://docs.google.com/document/d/1N7dRx5zwnyXCl0H-VuKpAHTbMBiZXRHozQ7pScSdz1s/edit)\n",
    "- [Kaggle tutorial: categorical variables](https://www.kaggle.com/code/alexisbcook/categorical-variables)\n",
    "- [Working with missing data](https://pandas.pydata.org/docs/user_guide/missing_data.html)\n",
    "- [Introduction to Quantization in PyTorch](https://pytorch.org/blog/introduction-to-quantization-on-pytorch/)\n",
    "- [Quantization in PyTorch](https://pytorch.org/docs/stable/quantization.html#module-torch.quantization)\n",
    "- [A Survey of Quantization Methods for Efficient Neural Network Inference](https://arxiv.org/pdf/2103.13630.pdf)\n",
    "- [Optuna](https://optuna.org/)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env_mlops",
   "language": "python",
   "name": "env_mlops"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
