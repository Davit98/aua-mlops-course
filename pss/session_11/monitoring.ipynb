{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "de6a7071-3908-45aa-95f4-fdd46b5d72f1",
   "metadata": {},
   "source": [
    "# AUA, DS 229 – MLOps\n",
    "### Week 14 – Monitoring Machine Learning Systems\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa4b40ca-aad7-42bb-8d67-b7aab2860dcf",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-danger\">\n",
    "<b>Action</b>:\n",
    "    <b>Open a terminal an run</b>: `docker-compose up --build`\n",
    "    then run all the cells of this notebook. Afterwards, you can continue \n",
    "    from the next cell.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7a4b439-dcf7-451b-9d93-0a7f8281f4d8",
   "metadata": {},
   "source": [
    "Machine learning systems are increasingly being used to solve complex problems across various fields, such as healthcare, finance, and autonomous vehicles. However, it is essential to monitor these systems to ensure that they are performing as expected and to identify any potential issues or biases.\n",
    "\n",
    "<center><img src=\"./images/ml_lifecycle.png\" width=900 height = 200/></center>\n",
    "\n",
    "[[Image source](https://martinfowler.com/articles/cd4ml.html)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e24a553f-dc9c-4b9e-875b-5dcf896a146b",
   "metadata": {},
   "source": [
    "#### Monitoring vs Testing\n",
    "\n",
    "**Testing** – Our best effort verification of correctness (**necessity**)  \n",
    "**Monitoring** – Our best effort to track predictable failures (**satisfaction**)\n",
    "\n",
    "[[Source](https://christophergs.com/machine%20learning/2020/03/14/how-to-monitor-machine-learning-models/)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bd34b82-3456-48f2-b950-61be09412aff",
   "metadata": {},
   "source": [
    "<mark>What problems can be detected as a result of monitoring?</mark>  \n",
    "Monitoring machine learning systems involves collecting data about the system's behavior and performance, analyzing the data to identify patterns and trends, and taking appropriate actions based on the findings. This process can help to detect and address issues such as model drift, data quality problems, and biases in the system."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dffbb041-c4ae-4851-b5e7-4427a3d245fa",
   "metadata": {},
   "source": [
    "### Data drift\n",
    "\n",
    "<center><img src=\"./images/data_drift.png\" width=700 height = 150/></center>\n",
    "\n",
    "Data drift can occur when the distribution of data used to train a machine learning model changes over time. This can happen for a variety of reasons, such as changes in user behavior, changes in the underlying environment, or changes in the data collection process. When data drift occurs, the performance of the machine learning model can degrade, as it is no longer making accurate predictions based on the new data.\n",
    "\n",
    "Concept drift detection is a technique used to monitor for data drift. It involves comparing the distribution of the data used to train the machine learning model to the distribution of the new data that the model is being used to make predictions on. If there is a significant difference between these distributions, it can indicate that data drift has occurred.\n",
    "\n",
    "There are various approaches to detecting concept drift, including statistical methods, distance-based methods, and density-based methods. Statistical methods involve comparing statistical properties of the data distributions, such as mean, variance, and correlation. Distance-based methods measure the difference between the distributions using distance metrics such as Kullback-Leibler divergence or Earth Mover's Distance. Density-based methods estimate the probability density function of the data and compare it to the previous distribution.\n",
    "\n",
    "Once data drift has been detected, there are several strategies that can be used to address it. One approach is to retrain the machine learning model using the new data. Another approach is to use techniques such as online learning or incremental learning to update the model in real-time as new data arrives. In some cases, it may also be necessary to collect additional data that better represents the new distribution."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fba7e0ad-18f4-4176-96a8-a426a36d7887",
   "metadata": {},
   "source": [
    "## The goal\n",
    "\n",
    "Of course, we are naturally interested in knowing how accurate our models are when they are put into use. However, it's often impossible to immediately assess the accuracy of a model in many cases. For example, with a fraud detection model, the accuracy of its predictions can only be confirmed when new live cases arise and are investigated or when customer data is cross-checked against known fraudsters. Other fields, such as disease risk prediction, credit risk prediction, future property values, and long-term stock market prediction, also face similar challenges where immediate feedback is not available. With these limitations in mind, it makes sense to track proxy values to model accuracy in production, including metrics like precision, recall, F1 score, or area under the receiver operating characteristic curve (AUC-ROC). As a proxy of model accuracy can play:\n",
    "- Model prediction distribution (regression algorithms) or frequencies (classification algorithms) – we can compare the distributions of our model predictions using statistical tests, either through an automated or manual process. This can involve basic statistical measures such as median, mean, standard deviation, and maximum/minimum values. For instance, if the variables follow a normal distribution, we would anticipate the mean values to fall within the standard error of the mean range. However, this is a simple statistical method and may not capture all of the nuances of the data. More in-depth analyses may be required to fully understand the accuracy of the model's predictions.\n",
    "\n",
    "- Model input distribution (numerical features) or frequencies (categorical features), as well as missing value checks – If we have a predetermined set of values for an input feature, we can perform two checks to ensure the model's accuracy. Firstly, for categorical inputs, we can confirm that the input values fall within an acceptable set. For numerical inputs, we can verify that the input values are within the expected range. Secondly, we can check that the frequency of each respective value within the set is consistent with what we have observed in previous data. By conducting these checks, we can confirm that the model is producing reliable predictions based on the input values.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e402ceba-9e2a-4c6f-9f05-d083af3d3cc7",
   "metadata": {},
   "source": [
    "## Operations Monitoring\n",
    "\n",
    "The field of software engineering has a more established system of monitoring which falls under the umbrella of Site Reliability Engineering. When it comes to the operational aspects of our ML system, the following areas are of concern:\n",
    "\n",
    "- System Performance (**Latency**) – the amount of time it takes for a system to respond to a given input or request. Latency can be measured in various units such as seconds, milliseconds, or microseconds.\n",
    "- System Performance (**IO/Memory/Disk Utilisation**) – IO Utilization refers to the amount of data being transferred to and from a system's storage devices, such as hard drives or solid-state drives (SSDs). High IO utilization can indicate that the system is heavily reliant on its storage devices and may be experiencing a bottleneck in data transfer. This can lead to slower response times and reduced overall system performance. Memory Utilization refers to the amount of physical memory being used by the system. Disk Utilization refers to the amount of disk space being used by the system. High disk utilization can indicate that the system is running out of storage space, which can impact performance and cause errors or crashes.\n",
    "- System Reliability (**Uptime**) – System reliability refers to the ability of a computer system to function without failure or interruption over a period of time. Uptime, in particular, is a measure of system reliability that refers to the amount of time a system is operational and available for use. Uptime is typically measured as a percentage of the total time a system is expected to be available. For example, a system that is expected to be available 24 hours a day, 7 days a week has a total uptime of 100%, while a system that experiences 1 hour of downtime in a week has an uptime of 99.6%.\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4687f3ee-a4e9-42e3-b60d-d017fae6d410",
   "metadata": {},
   "source": [
    "## [Prometheus](https://prometheus.io/docs/introduction/overview/)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15dd0d1c-c3f7-4dd3-ae2f-d647a1bbbbc4",
   "metadata": {},
   "source": [
    "<center><img src=\"./images/prometheus.png\" width=200 height = 80/></center>\n",
    "\n",
    "Prometheus is an open-source systems monitoring and alerting toolkit originally built at SoundCloud.\n",
    "\n",
    "**Prometheus collects and stores its metrics as time series data, i.e. metrics information is stored with the timestamp at which it was recorded, alongside optional key-value pairs called labels.**\n",
    "\n",
    "#### What are metrics?\n",
    "The metrics are numeric measurements. Time series means that changes are recorded over time. What users want to measure differs from application to application. For a web server it might be request times, for a database it might be number of active connections or number of active queries etc.\n",
    "\n",
    "Metrics play an important role in understanding why your application is working in a certain way. Let's assume you are running a web application and find that the application is slow. You will need some information to find out what is happening with your application. For example the application can become slow when the number of requests are high. If you have the request count metric you can spot the reason and increase the number of servers to handle the load.\n",
    "\n",
    "[[Source](https://prometheus.io/docs/introduction/overview/)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6babc82-4483-43da-b4bd-276ea3615a81",
   "metadata": {},
   "source": [
    "## [Grafana](https://grafana.com/grafana/dashboards/)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e254af2e-e522-4eb3-aa46-c70fbaebb293",
   "metadata": {},
   "source": [
    "<center><img src=\"./images/grafana.png\" width=400 height = 80/></center>\n",
    "\n",
    "Grafana is an open-source analytics and visualization platform that enables users to create, explore, and share dashboards, visualizations, and data queries. It allows users to connect to and visualize data from various sources, including databases, cloud platforms, and other data repositories, and build interactive, real-time dashboards for monitoring and analysis.\n",
    "\n",
    "Grafana provides an intuitive user interface and a wide range of visualization options, including graphs, charts, and tables, making it easy for users to explore and understand their data. It also supports advanced features like alerting, data transformations, and annotations, allowing users to detect and respond to anomalies and trends in their data.\n",
    "\n",
    "Grafana is highly extensible, with a rich plugin architecture that allows users to customize and extend the platform's functionality. It also integrates with a range of popular data sources and tools, including InfluxDB, Elasticsearch, Prometheus, and more.\n",
    "\n",
    "Grafana is widely used by organizations of all sizes and industries, from startups to large enterprises, to monitor and analyze their data and gain insights into their systems and operations. With its flexible architecture, powerful visualization capabilities, and active developer community, Grafana is a popular choice for building custom analytics and monitoring solutions.\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f17c58c-687d-4ea5-b8ef-0c3fa5bfa120",
   "metadata": {},
   "source": [
    "# Monitoring breast cancer predictor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f71132e-e3b1-4e04-8ba0-7915d0c0ca7e",
   "metadata": {},
   "source": [
    "#### Project struture:\n",
    "\n",
    "- We create a Connexion app instance and add the API definition as before.\n",
    "\n",
    "- We create a Flask app instance from the Connexion app, which allows us to access the underlying Flask application.\n",
    "\n",
    "- We use the DispatcherMiddleware class from the werkzeug.middleware.dispatcher module to add a Prometheus metrics endpoint to the Flask app. This endpoint will be accessible at */metrics*.\n",
    "\n",
    "- Finally, we start the application by calling app.run() as before. Now, the application will be monitored by Prometheus and you can view the metrics by accessing the */metrics* endpoint.\n",
    "\n",
    "*Prometheus will act as a data source where all the data will be available on port 9090, and Grafana will fetch these data points and display them in the dashboard*."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b20db6b-f399-4406-b2ce-d1af7f82fdb7",
   "metadata": {},
   "source": [
    "We are going to run all our services locally on Docker containers.\n",
    "- [Prometheus Docker Hub image](https://hub.docker.com/r/prom/prometheus/)\n",
    "- [Grafana Docker Hub image](https://hub.docker.com/r/grafana/grafana)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cc5fd84-1eba-4f41-b5e6-4a4a2378e589",
   "metadata": {},
   "source": [
    "The most important point in `docker-compose.yml` configuration is `prometheus.yml` file mounting from our local to the docker container. This file includes configuration for pulling data (metrics) from our app service or Python project. Without the file, we won't able to see the custom metrics that our project includes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fedd647-c44e-47f2-929f-813d4f18afbc",
   "metadata": {},
   "source": [
    "The Prometheus configuration file, `prometheus.yml`, is used to specify which targets Prometheus should scrape and how to process the collected data. By default, Prometheus uses the YAML file from path: `/etc/prometheus/prometheus.yml`. We need to add our own `prometheus.yml` file and inform Prometheus to use that one.\n",
    "\n",
    "The `prometheus.yml` file has a simple structure with three main sections:\n",
    "\n",
    "- **global**: This section contains global configuration settings for Prometheus. These settings apply to all scraping jobs defined in the configuration file. Some examples of global settings include the evaluation interval and the scrape timeout.\n",
    "\n",
    "    > The **scrape_interval** parameter specifies the interval at which Prometheus should scrape the metrics data from the target. The default value for scrape_interval is 1 minute, but you can adjust this interval according to your needs. It is important to note that **scrape_interval** affects both the performance and the accuracy of the metrics collected by Prometheus. A shorter interval will result in more frequent scraping and more up-to-date data, but can also put more load on the system being monitored. A longer interval will reduce the load on the system, but may also result in less accurate data.\n",
    "\n",
    "    > The **evaluation_interval** parameter specifies the interval at which Prometheus should re-evaluate the rules defined in its configuration files and generate alerts if any of the defined alert conditions are met. The default value for evaluation_interval is 1 minute, but you can adjust this interval according to your needs. A shorter interval will result in more frequent evaluations and more up-to-date alerts, but can also put more load on the system being monitored. A longer interval will reduce the load on the system, but may also result in less accurate and less timely alerts.\n",
    "\n",
    "\n",
    "- **scrape_configs**: This section defines the list of targets that Prometheus should scrape. Each target is defined in a separate block within the scrape_configs section. Each block specifies the URL endpoint that Prometheus should scrape, along with any additional configuration settings for that target. For example, you might specify the HTTP port, path, and metrics path for a target.\n",
    "\n",
    "    > The **job_name** parameter specifies the name of the job that the target belongs to. This name is used to identify the job in the Prometheus UI and in alert rules, making it easier to manage and monitor different targets and their associated metrics.  \n",
    "    Each **job_name** should have its own set of scrape_configs defined in the `prometheus.yml` configuration file. Each **scrape_config** defines the specific endpoints or URLs that Prometheus should scrape for metrics data for that particular job.\n",
    "\n",
    "\n",
    "- **rule_files**: This section defines the list of files containing Prometheus rules that should be evaluated. Each rule file is defined in a separate line. Rules are used to define alerts based on the collected metrics. For example, you might create a rule that generates an alert if a certain metric goes above a certain threshold.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b21048e7-5afc-4cce-b0b6-be8af139e321",
   "metadata": {},
   "source": [
    "#### [Prometheus metric types include](https://prometheus.io/docs/concepts/metric_types/):\n",
    "\n",
    "- **Counter**: A counter is a cumulative metric that counts the number of events that occur. **Counters always increase in value and never decrease**. They are useful for tracking the number of requests, errors, or other events. Do not use a counter to expose a value that can decrease. For example, do not use a counter for the number of currently running processes; instead use a gauge.  \n",
    "When working with counter metrics in Prometheus, two related metrics are generated automatically: one with the **_count** postfix and another with the **_created** postfix. The metric with the _count postfix represents the total number of events that have occurred since the counter was created or last reset. This metric is incremented every time an event is observed. For example, if you are monitoring the number of HTTP requests received by a web server, the _count metric would represent the total number of requests received. The metric with the _created postfix represents the timestamp when the counter was created. This metric is useful when you want to track how long a counter has been running. For example, if you are monitoring the uptime of a service, the _created metric would represent the timestamp when the service was started.\n",
    "\n",
    "\n",
    "\n",
    "- **Gauge**: A gauge is a metric that represents a single numerical value that can go up or down over time. Gauges are used to track metrics such as CPU usage, memory usage, or the number of connections to a database.\n",
    "\n",
    "- **Histogram**: A histogram is used to measure the distribution of a set of values. It separates values into buckets and counts how many values fall into each bucket. Histograms are useful for measuring things like response times or request sizes. A histogram with a base metric name of `basename` exposes multiple time series during a scrape:\n",
    "  - cumulative counters for the observation buckets, exposed as `<basename>_bucket` (each bucket of the histogram is represented by a separate time-series with a label **le** that indicates the upper bound of the bucket range.)\n",
    "  - the total sum of all observed values, exposed as `<basename>_sum`\n",
    "  - the count of events that have been observed, exposed as `<basename>_count`  \n",
    "  \n",
    "- **Summary**: A summary is similar to a histogram. While it also provides a total count of observations and a sum of all observed values, it calculates configurable quantiles over a sliding time window. Summaries are useful for measuring things like request latencies. A summary with a base metric name of `<basename>` exposes multiple time series during a scrape:\n",
    "  - streaming φ-quantiles (0 ≤ φ ≤ 1) of observed events, exposed as `<basename>`\n",
    "  - the total sum of all observed values, exposed as `<basename>_sum`\n",
    "  - the count of events that have been observed, exposed as `<basename>_count`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28cc0a08-b1fc-446d-8217-ffac7e0a9917",
   "metadata": {},
   "source": [
    "Histograms and summaries both sample observations, typically request durations or response sizes. They track the number of observations and the sum of the observed values, allowing you to calculate the average of the observed values. Note that the number of observations (showing up in Prometheus as a time series with a _count suffix) is inherently a counter (as described above, it only goes up). The sum of observations (showing up as a time series with a _sum suffix) behaves like a counter, too, as long as there are no negative observations. Obviously, request durations or response sizes are never negative. In principle, however, you can use summaries and histograms to observe negative values (e.g. temperatures in centigrade). In that case, the sum of observations can go down, so you cannot apply rate() to it anymore. In those rare cases where you need to apply rate() and cannot avoid negative observations, you can use two separate summaries, one for positive and one for negative observations (the latter with inverted sign), and combine the results later with suitable PromQL expressions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0661cdb-3ac5-4dfe-b006-8ca6572f5b20",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "Open http://localhost:8000/ui/ and http://localhost:3000/. For Grafana, use **admin** both for username and password. Then it will require adding a new password and we can keep it the same as it is since we're testing locally. Visit http://localhost:9090 for prometheus server.\n",
    "\n",
    "Add a Prometheus data source to Grafana by navigating to the Grafana web interface at http://localhost:3000. Click on \"Add data source\", select \"Prometheus\" as the type, and enter the URL of the Prometheus server (http://prometheus:9090 or http://host.docker.internal:9090 in this case). Click \"Save & Test\" to verify the connection.\n",
    "\n",
    "Open http://localhost:3000/dashboards and add a new panel by selecting the metric you want to monitor."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24068d89-1ce3-40b0-823d-de656f0b023d",
   "metadata": {},
   "source": [
    "> In most cases, specifying `localhost` or `127.0.0.1` in `prometheus.yml` will not work when the target is running inside a Docker container because the localhost address refers to the container itself, not the host machine.\n",
    "\n",
    "> Instead, you need to use a special DNS name, `host.docker.internal`, that resolves to the internal IP address of the host machine from within the Docker container. This DNS name was introduced in Docker version 17.12 for both Mac and Windows and can be used to access services running on the host machine from within a Docker container.\n",
    "\n",
    "> So, when running a service inside a Docker container that needs to be accessed from another service running on the host machine, you should use `host.docker.internal` instead of `localhost` or `127.0.0.1` in the configuration files."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b961b991-4cd1-40d8-a71a-734f28365dfa",
   "metadata": {},
   "source": [
    "### Simulating Live Data for Breast Cancer Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ec120cf-f272-40c9-ae06-5bbe44d4a746",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import time\n",
    "import random\n",
    "\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "998474ec-a48e-45c2-89db-fcec52fc5b43",
   "metadata": {},
   "outputs": [],
   "source": [
    "# These statistics are taken from the training data.\n",
    "stats = {\n",
    "    \"mean\": {\n",
    "        0: {\n",
    "            \"mean concavity\": 0.160775, \n",
    "            \"worst area\": 1422.286321, \n",
    "            \"mean area\": 978.376415\n",
    "        },\n",
    "        1: {\n",
    "            \"mean concavity\": 0.046058, \n",
    "            \"worst area\": 558.899440, \n",
    "            \"mean area\": 462.790196\n",
    "        }\n",
    "    },\n",
    "    \"std\": {\n",
    "        0: {\n",
    "            \"mean concavity\": 0.075019, \n",
    "            \"worst area\": 597.967743, \n",
    "            \"mean area\": 367.937978\n",
    "        },\n",
    "        1: {\n",
    "            \"mean concavity\": 0.043442, \n",
    "            \"worst area\": 163.601424, \n",
    "            \"mean area\": 134.287118\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "\n",
    "def sample_features(target=None):\n",
    "    \"\"\"Samples features corresponding to the target if specified.\"\"\"\n",
    "    \n",
    "    if target is None:\n",
    "        target = random.randint(0, 1)\n",
    "    \n",
    "    means = list(stats[\"mean\"][target].values())\n",
    "    stds = list(stats[\"std\"][target].values())\n",
    "\n",
    "    return [np.random.normal(mean, std / 10) for mean, std in zip(means, stds)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ee5ba15-8f20-43f0-9267-4b653007fd72",
   "metadata": {},
   "outputs": [],
   "source": [
    "minutes = 10  # For how many minutes to run each nested loop.\n",
    "URL = \"http://localhost:8000/predict\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb25441d-cfbd-460f-9f98-755488fb1f83",
   "metadata": {},
   "outputs": [],
   "source": [
    "for target in (0, 1) * 50:\n",
    "    \n",
    "    END_TIME = time.time() + 60 * minutes\n",
    "    current_min = 1\n",
    "    request_counter = 0\n",
    "    print(f\"Feeding data 'corresponding' to target {target}\")\n",
    "\n",
    "    # Nested loop start.\n",
    "    while time.time() < END_TIME:\n",
    "\n",
    "        sec = random.random() * 15\n",
    "        time.sleep(sec)\n",
    "\n",
    "        feature_values = sample_features(target=target)\n",
    "        params = {\n",
    "            \"mean_concavity\": feature_values[0], \n",
    "            \"worst_area\": feature_values[1],\n",
    "            \"mean_area\": feature_values[2]\n",
    "        }\n",
    "        response = requests.get(URL, params)\n",
    "\n",
    "        diff = END_TIME - time.time()\n",
    "        if (diff > 0) and (diff // 60 == minutes - current_min):\n",
    "            current_min += 1\n",
    "            print(f\"  .. {int(diff)} seconds still to go\")\n",
    "\n",
    "        request_counter += 1\n",
    "\n",
    "\n",
    "    print(\"=\"*40)\n",
    "    print(f\"Performed {request_counter} requests.\\n\\n\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f240622-20d4-4a85-a442-0a2918885d04",
   "metadata": {},
   "source": [
    "***\n",
    "## [PromQL](https://prometheus.io/docs/prometheus/latest/querying/basics/)\n",
    "\n",
    "Prometheus provides a functional query language called PromQL (Prometheus Query Language) that lets the user select and aggregate time series data in real time."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d382c00-ef2a-4206-95fc-922a432271c7",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-danger\">\n",
    "<b>Action</b>:\n",
    "    Visit http://localhost:3000/ and create a dashboard to visualize the following metrics:\n",
    "</div>\n",
    "\n",
    "#### 1) Visualize the number of predict requests as a time series data:\n",
    "> `request_count_total`\n",
    "\n",
    "#### 2) Visualize the predicted class labels:\n",
    "> `target_sum_sign`\n",
    "\n",
    "### [Range Vector Selectors](https://prometheus.io/docs/prometheus/latest/querying/basics/#range-vector-selectors)\n",
    "Range Vector Selectors select a range of samples back from the current instant. Syntactically, a time duration is appended in square brackets (`[]`) at the end of a vector selector to specify how far back in time values should be fetched for each resulting range vector element. In the first example below, we select all the values we have recorded within the last 5 minutes for all time series that have the metric name **request_processing_seconds_count** (paste the following two queries in http://localhost:9090/ query space and play with minutes):\n",
    "> `request_processing_seconds_count[5m]`  \n",
    "> `request_processing_seconds_sum[10m]`\n",
    "\n",
    "[Here](https://prometheus.io/docs/prometheus/latest/querying/basics/#time-durations) is the list of possible time units. Note that each observation in each of the two queries above are sperated from each other exactly by the amount of time specified in **scrape_interval** in `prometheus.yml`\n",
    "\n",
    "#### `rate()`, `irate()` and  `increase()`\n",
    "- `rate()`: This calculates the rate of increase per second, averaged over the entire provided time window.   \n",
    "Example: `rate(request_count_total[5m])` yields the per-second rate of HTTP requests as averaged over a time window of 5 minutes. \n",
    "- `irate()` (\"instant rate\"): This calculates the rate of increase per second just like `rate()`, but only considers the last two samples under the provided time window for the calculation and ignores all earlier ones.   \n",
    "Example: `irate(request_count_total[5m])` looks at the two last samples under the provided 5-minute window and calculates the per-second rate of increase between them. This function can be helpful if you want to make a zoomed-in graph show very quick responses to changes in a rate, but the output will be much more spiky than for `rate()`.\n",
    "- `increase()`: This function is exactly equivalent to `rate()` except that it does not convert the final unit to \"per-second\" (1/s). Instead, the final output unit is per-provided-time-window.   \n",
    "Example: `increase(request_count_total[5m])` yields the total increase in handled HTTP requests over a 5-minute window (unit: 1 / 5m). Thus `increase(foo[5m]) / (5 * 60)` is 100% equivalent to `rate(foo[5m])`.\n",
    "\n",
    "See how is the rate calculated [here](https://promlabs.com/blog/2021/01/29/how-exactly-does-promql-calculate-rates/).\n",
    "\n",
    "\n",
    "#### 3) Calculate the average number of predict requests per 10m:\n",
    "> `increase(request_count_total[10m])`\n",
    "\n",
    "\n",
    "#### 4) Calculate the average request time in seconds during the last 2 minutes:\n",
    "> `(rate(request_processing_seconds_sum[2m]) / rate(request_processing_seconds_count[2m]))`\n",
    "\n",
    "#### 5) Calculate the average `worst_area` during the last 2 minutes:\n",
    "> `(rate(worst_area_hist_sum[2m]))/(rate(worst_area_hist_count[2m]))`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4718e7ea-0e10-4cbe-9f2e-9822afe981b5",
   "metadata": {},
   "source": [
    "# References\n",
    "- [Prometheus](https://prometheus.io/docs/introduction/overview/)\n",
    "- [Grafana](https://grafana.com/docs/grafana/latest/)\n",
    "- [PromQL intro](https://prometheus.io/docs/prometheus/latest/querying/basics/)\n",
    "- [PromQL functions](https://prometheus.io/docs/prometheus/latest/querying/functions/#rate)\n",
    "- [How Exactly Does PromQL Calculate Rates?](https://promlabs.com/blog/2021/01/29/how-exactly-does-promql-calculate-rates/)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env_mlops",
   "language": "python",
   "name": "env_mlops"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
