{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bcdf2b29-2725-4093-9dc2-fd9bcdf1a5e5",
   "metadata": {},
   "source": [
    "# AUA, DS 229 – MLOps\n",
    "### Week 6 – Testing ML systems with `pytest` and `tox`\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "968fab4f-8f29-45b5-8ec3-5692b1db19f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "567a6c52-4b4a-4b72-8f39-85d4bb8b8b85",
   "metadata": {
    "tags": []
   },
   "source": [
    "### `==` vs `is`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ad7743e-cb34-4643-a658-9cdf2a191ac5",
   "metadata": {},
   "source": [
    "- `==` is for value equality. It's used to know if two objects have the same value.\n",
    "- `is` is for reference equality. It's used to know if two references refer (or point) to the same object, i.e if they're identical. Two objects are identical if they have the same memory address.\n",
    "\n",
    "**Two objects having equal values are not necessarily identical.**  \n",
    "For short, `==` determines if the values of two objects are equal, while `is` determines if they are the exact same object.\n",
    "\n",
    "The `id()` function returns **a unique id for the specified object**. <mark>All objects in Python have their own unique ids. The id is assigned to the object when it is created and represents object's memory address.</mark>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38da854e-5621-4841-bc05-5e6833ae6ea0",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = [1, 2, 3]\n",
    "b = a\n",
    "\n",
    "print(f\"a == b: {a == b}\")\n",
    "print(f\"a is b: {a is b}\")\n",
    "print(\"-\"* 15, f\"\\nid(a): {id(a)}\")\n",
    "print(f\"id(b): {id(b)}\")\n",
    "print(f\"id(a) == id(b): {id(a) == id(b)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21660614-7180-436c-b1be-40d4912c192f",
   "metadata": {},
   "outputs": [],
   "source": [
    "c = a[:]  # c = [1, 2, 3]\n",
    "\n",
    "print(f\"a == c: {a == c}\")\n",
    "print(f\"a is c: {a is c}\")\n",
    "print(\"-\"* 15, f\"\\nid(a): {id(a)}\")\n",
    "print(f\"id(c): {id(c)}\")\n",
    "print(f\"id(a) == id(c): {id(a) == id(c)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f31c289-c53b-4a93-a5df-28a78baf1a30",
   "metadata": {},
   "source": [
    "#### Ambiguities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95ab1d9a-fd28-41db-af12-9bb847f72737",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = 1000\n",
    "b = 1000\n",
    "\n",
    "print(f\"a == b: {a == b}\")\n",
    "print(f\"a is b: {a is b}\")\n",
    "print(\"-\"* 15, f\"\\nid(a): {id(a)}\")\n",
    "print(f\"id(b): {id(b)}\")\n",
    "print(f\"id(a) == id(b): {id(a) == id(b)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90dea6dc-0c52-405e-9b16-25037d88d33e",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = 256\n",
    "b = 256\n",
    "\n",
    "print(f\"a == b: {a == b}\")\n",
    "print(f\"a is b: {a is b}\")\n",
    "print(\"-\"* 15, f\"\\nid(a): {id(a)}\")\n",
    "print(f\"id(b): {id(b)}\")\n",
    "print(f\"id(a) == id(b): {id(a) == id(b)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "599e0303-3bc2-4035-b086-f9f441a3f175",
   "metadata": {},
   "source": [
    "<mark>The reason for so is that Python caches integer objects in the range $[-5, 256]$ as singleton instances for performance reasons.</mark>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57740717-cb90-45cd-a40f-37b58a2d3c41",
   "metadata": {},
   "source": [
    "[PEP8](https://peps.python.org/pep-0008/#programming-recommendations) styleguide suggests that **comparisons to singletons like None should always be done with `is` or `is not`, never the equality operators**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d24f8749-81ef-4ba4-8937-35436460cd56",
   "metadata": {
    "tags": []
   },
   "source": [
    "### [**pytest**](https://docs.pytest.org/en/7.2.x/)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27761657-89eb-4249-bd97-f2353fb9f04a",
   "metadata": {},
   "source": [
    "The structure of today's toy project:\n",
    "\n",
    "```\n",
    "├── source/\n",
    "│   ├── __init__.py\n",
    "│   └── softmax.py\n",
    "│\n",
    "├── tests/\n",
    "│   ├── __init__.py\n",
    "│   └── test_softmax.py\n",
    "│\n",
    "├── pytest.ini\n",
    "├── tox.ini\n",
    "└── requirements.txt\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c31369ee-d73e-4482-8d5c-e73da478f37e",
   "metadata": {},
   "source": [
    "Let's examine the **stable_softmax** function in `source/softmax.py`. It implements stable softmax function by taking inputs an array $x$ (either numpy or torch) and a constant value $const$ and applying softmax on $x-const$. For $x = [x_1, x_2, \\dots, x_n]$, \n",
    "\n",
    "$$\n",
    "\\text{softmax}(x)_i = \\frac{\\exp{(x_i)}}{\\sum_{k=1}^n \\exp{(x_k)}},\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\text{softmax}(x + c) = \\text{softmax}(x),\n",
    "$$\n",
    "\n",
    "where $c$ is some constant. It is empirically proven that picking $const := -\\max(x)$ helps avoiding numerical overflow. \n",
    "\n",
    "In our implementation of **stable_softmax** we keep `const` as a free argument for demonstrating certain `pytest` functionalities. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4823ea93-a8d3-4332-b616-cc76c7f6f1fa",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-danger\">\n",
    "<b>Action</b>:\n",
    "    Go through <b>source/softmax.py</b>.\n",
    "</div> "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24c321c2-cebe-403b-9a99-4e2d5c6737ee",
   "metadata": {},
   "source": [
    "`pytest` is a framework for testing code written in python in a more flexible and easier manner compared to `unittest`.\n",
    "\n",
    "When `pytest` is run in a terminal, it will search all directories below where it was called, find all of the Python files in these directories whose **names start with `test_` or end with `_test`**, import them, and run all of the functions and classes whose **names start with `test` or `Test`**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e7588cb-051b-47e1-a1cc-b396a8928367",
   "metadata": {},
   "source": [
    "However, to inject custom behavior, we can specify pytest settings either in `pytest.ini` or `pyproject.toml` files. These files by convention reside in the root directory of your repository.\n",
    "\n",
    "In our codebase we have `pytest.ini` file of the following structure:\n",
    "- **testpaths**: Sets list of directories that should be searched for tests when no specific directories, files or test ids are given in the command line when executing pytest from the rootdir directory. Useful when all project tests are in a known location to speed up test collection and to avoid picking up undesired tests by accident.\n",
    "- **python_files**: One or more glob-style file patterns determining which python files are considered as test modules.\n",
    "- **markers**: When the `--strict-markers` or `--strict command-line` arguments are used, only known markers defined in code by core pytest or some plugin are allowed. You can list additional markers in this setting to add them to the whitelist, in which case you probably want to add `--strict-markers` to avoid future regressions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee689c96-ef0a-4a55-b32a-742843085d6f",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-danger\">\n",
    "<b>Action</b>:\n",
    "    Uncomment and run <b>SIMPLE TEST</b> from <b>tests/test_softmax.py</b>.\n",
    "</div> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a020c50-858e-4192-8ba2-f20d094e1eaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pytest --verbose\n",
    "\n",
    "# !pytest tests/test_softmax.py --verbose\n",
    "# !pytest tests/ --verbose\n",
    "# !pytest tests/test_softmax.py::test_softmax_numpy_multiple_1d --verbose"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6476df2-e7e0-4377-9016-b31c3e76c9a7",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-danger\">\n",
    "<b>Action</b>:\n",
    "    Uncomment and run <b>MULTIPLE TESTS</b> from <b>tests/test_softmax.py</b>.\n",
    "</div> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2d1a420-1eda-4b1b-8464-21fce294f954",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pytest --verbose"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8e25a43-6ce8-401e-9b5d-2a2aa831eefe",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-danger\">\n",
    "<b>Action</b>:\n",
    "    Uncomment and run <b>MULTIPLE TESTS (PARAMETRIZATION)</b> from <b>tests/test_softmax.py</b>.\n",
    "</div> "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e164fe0-553e-4f93-a87d-56e548478b17",
   "metadata": {},
   "source": [
    "The builtin **pytest.mark.parametrize** decorator enables parametrization of arguments for a test function. As a first argument it receives testing function argument names in a string separated by commas. Next, it expects a list of tuples where each tuple represents testing function argument values. Then the underlying testing function is run as many times as the number of tuples is."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b65aafc-f4cd-4829-b7cf-b977ab1313d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pytest --verbose"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bad40df-b5f8-42b0-a6e2-59ab72472cf1",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-danger\">\n",
    "<b>Action</b>:\n",
    "    Uncomment and run <b>MULTIPLE TESTS (COMBINATIONS)</b> from <b>tests/test_softmax.py</b>.\n",
    "</div> "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ff3c899-fd15-4756-9b5e-19246ee9aa7c",
   "metadata": {},
   "source": [
    "When applied **pytest.mark.parametrize** decorators multiple times, the testing function is run for all combinations of arguments. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbd53a61-92b1-412d-b7d3-09826c682a11",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pytest --verbose"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a8aab39-b7f3-4b98-9eed-3578ecadb823",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-danger\">\n",
    "<b>Action</b>:\n",
    "    Uncomment and run <b>BAD INPUTS</b> from <b>tests/test_softmax.py</b>.\n",
    "</div> "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9a9f68d-50f7-45a1-88c3-078c12d33cb1",
   "metadata": {},
   "source": [
    "To write assertions about raised exceptions, you can use **pytest.raises()** as a **context manager**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80d94024-a7eb-4def-a120-6a4fd0ff1c0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pytest --verbose"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96e0791c-e7f2-4b97-b82e-6df4f92affd7",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-danger\">\n",
    "<b>Action</b>:\n",
    "    Uncomment and run <b>MULTIPLE TESTS (FIXTURES)</b> from <b>tests/test_softmax.py</b>.\n",
    "</div> "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6ef2a5d-493f-481f-8861-e62b94676658",
   "metadata": {},
   "source": [
    "If you notice that you're creating multiple tests that **rely on the same set of data**, then it's likely that you'll need to use a fixture. Essentially, you can gather the shared data into a single function and decorate it with **@pytest.fixture** to signify that it's a fixture in pytest. This will allow you to use the same data as input for multiple test functions.\n",
    "\n",
    "Pytest only caches one instance of a fixture at a time, which means that when using a parametrized fixture, pytest may invoke a fixture more than once in the given scope."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faa62b6a-12da-4dda-998c-70a24c5a8876",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pytest --verbose"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43ab304b-1f38-41ff-abb4-c2b72837127b",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-danger\">\n",
    "<b>Action</b>:\n",
    "    Run <b>pytest --cov source</b> to view the test coverage report.\n",
    "</div> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03ee7ce5-aece-4476-ab4c-44596df53fac",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pytest --cov source"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74ed93a8-2f1e-46f3-bf66-f489c486dcd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pytest --cov source --cov-report html  # This will generate a more detailed and user-friendly report."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d203dcf-6e33-4d97-815f-748ca1aab53a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inside htmlcov directory you can find report files for each python script.\n",
    "# Let's open the one for softmax.py:\n",
    "!open htmlcov/d_411ffd8adc737496_softmax_py.html"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3882665-d392-4f8b-9454-3838d1065b4d",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-danger\">\n",
    "<b>Action</b>:\n",
    "    Uncomment and run <b>PYTORCH INPUTS</b> from <b>tests/test_softmax.py</b> and check the coverage again.\n",
    "</div> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c676877-f74a-4e71-811f-246f8195e273",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pytest --verbose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f85d1aac-d199-4526-8d47-8a2fb4bfa20f",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pytest --cov source"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30831c44-4520-4bd4-9370-61551e9d84b0",
   "metadata": {},
   "source": [
    "<center><img src=\"./images/dijkstra.jpeg\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e82411a-e1a8-4e59-a2c3-9eb225239560",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-danger\">\n",
    "<b>Action</b>:\n",
    "    Uncomment and run <b>MARKERS</b> from <b>tests/test_softmax.py</b>.\n",
    "</div> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c049f2b9-346f-4750-8c96-18ca9c4bfc51",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pytest --verbose"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7a9c3ae-0b79-42aa-af03-6a3b7cd16209",
   "metadata": {},
   "source": [
    "To use custom markers correctly, we must specify the newly-created markers in the **pyproject.toml** file. We can use the **--strict-markers** flag to ensure that all markers are defined in this file. Then, we need to declare our markers in markers list, providing some description on them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba6533e7-11c5-4c10-a58c-dc966eca2928",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pytest --markers  # Lists all the defined markers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4c1fab3-43cb-4e63-b2cf-a6da7a614aa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pytest -m \"small\"  # Runs all tests that are marked with 'small'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e29234e7-2391-4272-a565-a844f36fdc2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pytest -m \"not small\"  # Runs all tests that are NOT marked with 'small'."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51342d3d-aa39-4bf4-a0ca-f0f7db7db65b",
   "metadata": {
    "tags": []
   },
   "source": [
    "### [**tox**](https://tox.wiki/en/latest/)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83629725-5afe-4700-8788-a4176170fb75",
   "metadata": {},
   "source": [
    "`tox` is used to **automate Python tests** in a Machine Learning pipelines or in general Python projects. It is an open-source project that provides a convenient way to run commands in isolated environments. What tox does in the background can be roughly split into four main steps:\n",
    "1) Creates a virtual environment.\n",
    "2) Installs dependencies in the virtual environment.\n",
    "3) Runs commands.\n",
    "4) Returns output (for each environment created).\n",
    "\n",
    "<center><img src=\"./images/tox-diagram.png\" width=900 height = 900/></center>\n",
    "\n",
    "The steps 1 to 3 are run according to what the user provides in a config file. There are three approaches to configure tox. The first one is by adding a `tox.ini` file in the root of the project. The second one is by adding a `tool.tox` section in the `pyproject.toml` configuration file. The third one is by adding a `setup.cfg` file. We will move forward with `tox.ini` option.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db918a48-9a83-46c0-ae19-78cdb507db35",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-danger\">\n",
    "<b>Action</b>:\n",
    "    Take a look at <b>tox.ini</b> file.\n",
    "</div> \n",
    "\n",
    "Each executable block can be identified by square brackets. The first part is what we called global settings, which are contained in the first section called `[tox]`. Under that section there is an item `envlist`, which tells tox which environments to create when we run tox from the command line (step 1). The section also contains `skipsdist` item which is set to True when we are not testing a softawre package (don't have `setup.py`). The second section, `[testenv]`, tells tox what dependencies to install in our environments (step 2). This is specified in the `deps` variable. Then, with the environments we created and the dependencies installed, we tell tox to run he specified command for the step 3. In our example file we do the fllowing:\n",
    "1) Create environments with Python 3.7.14 and Python 3.8.14.\n",
    "2) Install *pytest, numpy, torch* and *scipy* in each of the environemnt. \n",
    "3) Tell tox to run `pytest --verbose`.\n",
    "\n",
    "\n",
    ">Note that your device must support the versions of python listed in the first step. For example, since I mange my python environments with `pyenv`, I run the following commands to install Python 3.7.14 and Python 3.8.14 interpretators: \n",
    ">- `pyenv install -v 3.7.14`\n",
    ">- `pyenv install -v 3.8.14`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63ed1760-30ab-4ccc-906d-134132e0b17c",
   "metadata": {},
   "source": [
    "To run a `tox.ini` file from the root of the working directory, we simply run `tox` from the terminal. This means that **tox will create the new environments**, **install the dependencies**, and **run the commands** provided in the configuration. Then it caches the environments to save time for later usage.\n",
    "\n",
    "<div class=\"alert alert-block alert-danger\">\n",
    "<b>Action</b>:\n",
    "    Run <b>tox</b>.\n",
    "</div> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7da07c55-8e49-46c4-b097-b42d7f5c5297",
   "metadata": {},
   "outputs": [],
   "source": [
    "!tox"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d48d0c69-c24b-439f-8002-fce5a4bed207",
   "metadata": {},
   "source": [
    "There are multiple variations of `tox` command, below are listed some of them:\n",
    "- `tox -e python3.7.14`: run against specified environments (for example, Python 3.7.14).\n",
    "- `tox -i https://pypi.my-alternative-index.org`: force to use an alternative URL address to download packages.\n",
    "- `tox --parallel --recreate python3.7.14 python3.8.14`: create and run multiple virtual environments in parallel (for example, to run python3.7.14 and python3.8.14 in parallel).\n",
    "- `tox --parallel-live --recreate python3.7.14 python3.8.14`: show the output of the parallel environments mentioned above."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94822e05-82d6-461e-95f5-e55c9359f617",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-danger\">\n",
    "<b>Action</b>:\n",
    "    Modify <b>tox.ini</b> file to install <b>requirements.txt</b> and run <b>tox</b> again.\n",
    "</div> \n",
    "\n",
    "```\n",
    "[tox]\n",
    "envlist = python3.7.14, python3.8.14\n",
    "skipsdist = true\n",
    "\n",
    "[testenv]\n",
    "deps = -rrequirements.txt\n",
    "commands = pytest --verbose\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ddd06f6-dbf7-4b75-a556-c19eabf12172",
   "metadata": {},
   "outputs": [],
   "source": [
    "!tox"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e5f0868-fe67-46c9-9256-ea0729d6b651",
   "metadata": {},
   "source": [
    "Within `tox.ini` one can define custom environments with their dependencies, run various commands (like in terminal), set environment variables, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1ed0a30-025f-45da-ab34-951da79f09a8",
   "metadata": {},
   "source": [
    "**<mark>IMPORTANT</mark>**  \n",
    "Using an outdated tox virtual environment can pose a risk. Tox has a limitation in that it cannot detect modifications in the dependencies mentioned in the `setup.py` and/or `requirements.txt` files. It's important to keep this in mind and to recreate the environments by using the `tox -r` flag whenever such changes are made. This will ensure that the environments are refreshed and up-to-date.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a21a495-acbd-409d-8127-2531248b154c",
   "metadata": {},
   "outputs": [],
   "source": [
    "!tox -r"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "547aa84f-6377-4d27-950a-7dc5c66af5c7",
   "metadata": {},
   "source": [
    "# References\n",
    "- [Testing Machine Learning Systems: Code, Data and Models](https://madewithml.com/courses/mlops/testing/#behavioral-testing)\n",
    "- [A simple tox.ini / default environments](https://tox.wiki/en/3.26.0/example/basic.html)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env_mlops",
   "language": "python",
   "name": "env_mlops"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
